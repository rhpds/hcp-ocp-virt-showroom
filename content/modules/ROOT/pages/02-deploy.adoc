= Deploy a Hosted Cluster

== Introduction

This module introduces you to the basics of using Advanced Cluster Management for Kubernetes (RHACM) and the Multicluster Engine operator.

This module will walk you through the steps required to deploy an OpenShift on OpenShift Virtualization cluster using hosted control planes as the provider.

*Goals*

* Deploy a new OpenShift cluster using a hosted control plane.
* Review the hosted cluster environment.

As a reminder, here are your credentials for the OpenShift Console:

Access the {openshift_cluster_console_url}[OpenShift cluster console^] with your web browser.

Administrator login:

* *User:* `{openshift_cluster_admin_username}`
* *Password:* `{openshift_cluster_admin_password}`

[[deploy-prereqs]]
== Prerequisites for Deployment

. Click on the link above to launch the OpenShift console in a new browser tab.
+
image::deploy/console_login.png[link=self, window=blank, width=100%]

. When you login you will be presented with a pop-up window that promotes the ease of managing clusters with RHACM.
Click the *x* in the corner to close the window.
+
image::deploy/cluster_create_popup.png[link=self, window=blank, width=100%]

. Your initial landing page will be on the ACM default view for All Clusters. Currently the only cluster being managed is the local cluster that we are running on.
[arabic]
.. Click on the *local-cluster* to find out more information about it.
+
image::deploy/acm_default_window.png[link=self, window=blank, width=100%]

. The *Overview* tab for the local cluster will show:
[arabic]
.. status of the environment
.. console URL for direct login
.. hosting environment
.. current release information
.. number of nodes and applications currently deployed on the cluster
+
image::deploy/local_cluster_overview.png[link=self, window=blank, width=100%]
+
. Examine the Add-ons.
[arabic]
.. Click on the *Add-ons* tab.
.. In the list you will see the *hypershift-addon* listed.
This is required for the deployment of hosted control planes.
+
image::deploy/local_cluster_addons.png[link=self, window=blank, width=100%]

. To use an infrastructure provider, credentials are required.
Credentials for OpenShift Virtualization on local-cluster has been pre-populated for use in this lab.
[arabic]
.. Click on *Credentials* in the left-side menu.
Note that there is a credential available called *kubevirt-secret*.
.. Click on the *kubevirt-secret* credential.
+
image::deploy/view_credentials.png[link=self, window=blank, width=100%]

. The *kubevirt-secret* credential is displayed and contains:
[arabic]
.. a pull-secret which gives access to the OpenShift registry for cluster installation,
.. a public ssh key from the bastion host which is useful in managing the hosted cluster once it's deployed.
+
image::deploy/kubevirt_secret_details.png[link=self, window=blank, width=50%]

. RHACM deploys and manages clusters by means of pods and data in OpenShift projects.
Create a dedicated project on the local-cluster to manage the hosted clusters.
[arabic]
.. Click on the *All Clusters* menu item at the top.
.. Select *local-cluster* from the drop down menu.
+
image::deploy/all_clusters_dropdown.png[link=self, window=blank, width=50%]
.. Click *Home* on the left-side menu.
.. Click *Projects* from the left-side menu.
.. Click on the *Create Project* button.
+
image::deploy/create_project.png[link=self, window=blank, width=100%]
.. Name the project *clusters*.
.. Click the *Create* button.
+
image::deploy/create_project_details.png[link=self, window=blank, width=50%]

. Cluster deployment is managed by RHACM, not the local-cluster.
Return to RHACM console.
[arabic]
.. Click on the *local-cluster* menu item at the top.
.. Select *All Clusters* from the drop-down menu.
+
image::deploy/return_to_acm.png[link=self, window=blank, width=80%]

[[deploy-cluster]]
== Deploy a hosted cluster

. Begin the deployment process.
[arabic]
.. Click on the blue *Create cluster* button
+
image::deploy/create_cluster.png[link=self, window=blank, width=100%]

. There is a choice of infrastructure providers.
Note that the Red Hat OpenShift Virtualization provider has the additional tag, *Saved Credentials*.
If you had added credentials for other infrastructure providers, they would also be so tagged.
[arabic]
.. Click the tile for *Red Hat OpenShift Virtualization*.
+
image::deploy/infrastructure_provider.png[link=self, window=blank, width=100%]

. All clusters provided by Red Hat OpenShift Virtualization are hosted control planes.
The tile here called *Hosted* mentions the benefits of using hosted control planes to deploy your OpenShift cluster.
[arabic]
.. Click the *Hosted* tile.
+
image::deploy/control_plane_type_hosted.png[link=self, window=blank, width=50%]

. The *Create cluster - Cluster details* window appears.
There are number of options to fill out for creating your new hosted cluster.
Most interestingly, one can deploy a variety of versions of OpenShift.
Specific versions can be added, as well, for consistency of vetted versions across the fleet.
+
CAUTION: Be sure to select the *Release image #version 4.17.xx#*, so the cluster can be upgraded in a later module.
+
[arabic]
.. *Infrastructure provider credential:* kubevirt-secret
.. *Cluster name:* `my-hosted-cluster`
.. *Cluster set:* default
.. *Release image:* #OpenShift *4.17.xx*#
.. *Etcd storage class:* ocs-external-storagecluster-ceph-rbd
+
CAUTION: Network changes: This workshop platform is self-hosted, and the cluster and service networks that would ordinarily be used for the hosted cluster are used by the hosting cluster. The YAML configuration of the cluster and service networks for the hosted cluster must be changed.
+
.. Click the *YAML On* switch
.. Change lines 27 through 31 (approximately) to match the following IP address ranges.
+
[,yaml,role=execute,subs="attributes"]
----
  networking:
    clusterNetwork:
      - cidr: 10.136.0.0/14
    serviceNetwork:
      - cidr: 172.32.0.0/16
----
.. You may close the YAML editor, by clicking *YAML: Off*, to give you some more room to work.
.. Click on *Next* after subsituting the values.
+
image::deploy/create_cluster_details.png[link=self, window=blank, width=100%]

. The *Create cluster - Node pools* window appears.
Node pools organize are the virtual machines that make up the compute node pool.
[arabic]
.. *Node pool name:* `my-node-pool`
.. *Node pool replica:* 2
.. *Core:* 2
.. *Memory (GiB):* 8
.. *Auto repair:* True
.. Click on *Next* after filling out the options.
+
image::deploy/create_node_pools.png[link=self, window=blank, width=100%]

. The *Create cluster - Storage mappings* window appears.
This lab does not require any additional or custom storage, so we will not use any storage mappings.
[arabic]
.. Click *Next*.
+
image::deploy/create_storage_mappings.png[link=self, window=blank, width=50%]

. The *Create cluster - Review* window appears.
Review the deployment configuration of the *Cluster details* and the *Node pools* that you have configured.
[arabic]
.. Examine the configuration and click the *Create* button to begin provisioning the hosted cluster.
+
image::deploy/create_cluster_review.png[link=self, window=blank, width=100%]

. A message that the cluster is starting deployment appears, and then will forward to the overview page for the hosted cluster.
+
image::deploy/hosted_cluster_overview.png[link=self, window=blank, width=100%]
+
NOTE: Please be patient while the cluster deploys.
+
. After 15-20 minutes the cluster deployment will be complete.
+
image::deploy/hosted_cluster_deploy_complete.png[link=self, window=blank, width=100%]

In the meantime, you can explore the control plane pods.

=== Control Plane Pods and Node Virtual Machines

The foregoing Introductory module indicated that in a hosted control plane environment, there are no dedicated control plane nodes in a hosted OpenShift environment.
All of the control plane processes run within containers, within a project on the cluster.

. Explore the control plane pods.
[arabic]
.. At bottom of the *Control plane status* section, there is a link to *Control plane pods*.
Click on the *Control plane pods* link.
+
image::deploy/control_plane_pods.png[link=self, window=blank, width=100%]

. A new browser tab opens displaying the pods in the project *clusters-my-hosted-cluster*.
The pods here will consist of mostly replica sets for the processes that run the hosted cluster control plane.
[arabic]
.. Search for pods by *Label*, indicating `app=etcd`.
.. Notice that they are configured as *StatefulSets*.
.. Click the *etcd-0* pod to details.
+
image::deploy/etcd_pods.png[link=self, window=blank, width=100%]

. The details page for the pod appears.
.. Notice the *node* it is assigned to.
The other etcd pods will show that they are assigned to different nodes through anti-affinity rules.
.. There is also a *pod-disruption budget* set so that the maximum unavailable at any time is 1, or the cluster becomes unavailable.
+
image::deploy/etcd0_details.png[link=self, window=blank, width=100%]

. Click the browser *back button* to return to the list of pods available in the *clusters-my-hosted-cluster* project.
[arabic]
.. Delete the label filter for *etcd-0* pod by clicking the *x*.
.. Enter a new label filter: `kubevirt.io=virt-launcher`
.. The pods named *virt-launcher* appear.
.. These are the *VirtualMachineInstance* pods, in your node pool.
.. Notice that the amount of *reserved memory* is much larger than any other pod in this project.
+
image::deploy/nodepool_pods.png[link=self, window=blank, width=100%]

. Examine the *VirtualMachineInstance* pods.
[arabic]
.. *Click on one of the pods* to bring up its detail page.
.. Notice the *node* that it is assigned to.
.. Notice the *pod-disruption budget*
.. Also notice the a *node selector*, which indicates the nodes that can host VMs
.. There are also potential differences in *restart policy*.
+
image::deploy/vmi_pods.png[link=self, window=blank, width=100%]

. Once done exploring the pods in the *clusters-my-hosted-cluster* project
[arabic]
.. Close that browser tab to return to the *my-hosted-cluster - Overview*.
+
. The provisioning process also deploys virtual machines to implement the node pool.
While the cluster is provisioning, watch how virtual machines are deployed.
[arabic]
.. On the left, click *Virtual machines*.
.. They will each have a unique name, prefixed by the node pool name.
.. They will each, eventually, have an IP address on the pod network.
+
image::deploy/acm_virtual_machines.png[link=self, window=blank, width=100%]
.. Explore the virtual machines, noting how some links are marked with a box-arrow, which will take you to the local-cluster's OpenShift Console details about the virtual machine.

[[explore-cluster]]
== Explore the Cluster

With the cluster deployed, start exploring by scrolling down the page.
The page presents information about the cluster itself, including how to login, the node-pools where application workloads will run, and the pods that support the control plane.

=== Cluster Details and Status

The next major section of the overview page provides details about the cluster deployed.

. Examine the details about the cluster.
Note the following details about the cluster:
[arabic]
.. Under *All Cluster* click the left menu item *Infrastructure -> Clusters*.
.. Now click the *my-hosted-cluster* link.
.. Scroll down the page to *Cluster details*
.. *Status* of the cluster
.. Details about its *infrastructure*
.. Where to manage future operations like *upgrades*.
. Note access information:
[arabic,start=4]
.. *cluster API address*
.. *console URL*
.. the *credentials*, which are conveniently hidden for security purposes.
+
image::deploy/hosted_cluster_details.png[link=self, window=blank, width=100%]

. To display the credentials:
[arabic]
.. Click the *eye icon*, it will provide the information for the *kubeadmin* user and a randomly generated password.
+
image::deploy/reveal_creds.png[link=self, window=blank, width=100%]

. To access the console:
[arabic]
.. Click on the *copy icon next to the password* to save it to the clipboard.
.. Click on the *Console URL* right above that.
+
image::deploy/copy_password.png[link=self, window=blank, width=100%]

. A new tab will open.
The browser security certificate prompts will appear.
[arabic]
.. Click the *Advanced* button.
.. Then the secondary warning, which differs from browser to browser.
+
image::deploy/browser_security.png[link=self, window=blank, width=100%]
+
NOTE: This must be done twice.

. Once certificate prompts are bypassed, the login to the OpenShift console appears.
[arabic]
.. Enter the username `kubeadmin`.
.. Paste the *password* from the clipboard.
.. Click *Log in*.
+
image::deploy/hosted_cluster_login_prompt.png[link=self, window=blank, width=100%]

. The OpenShift console home page appears, just as if logging into any other OpenShift cluster.
[arabic]
.. Notice that the *infrastructure provider is listed as KubeVirt*.
.. Note that the environment was newly provisioned as there are *59 days left on a default self-support trial*.
.. If desired, continue to explore the environment at your leisure.
When finished, the switch back to the hosting cluster tab to return to the remaining lab tasks.
+
image::deploy/hosted_cluster_console_home.png[link=self, window=blank, width=100%]

. Back on the hosted cluster overview, view the cluster summary.
[arabic]
.. Scroll down the page to the *Status* section
.. This is a summary describing the *number of nodes* and *applications* currently running in the cluster.
.. Click on the *large number above applications*
+
image::deploy/hosted_cluster_status.png[link=self, window=blank, width=100%]

. Examine all the applications currently running in the cluster.
[arabic]
.. A page appears that shows *all of the apps* currently running in the cluster.
+
image::deploy/hosted_cluster_applications.png[link=self, window=blank, width=100%]
+
At this point all the applications are administrative applications responsible for providing cluster services.
One could use the blue *Create application* button to configure centralized application deployments from RHACM to a hosted cluster fleet using ArgoCD.
+
image::deploy/hosted_cluster_create_app.png[link=self, window=blank, width=100%]
+
. When done exploring this page, *click the back button on your browser* to return to the *my-hosted-cluster Overview*.

=== Nodes and Node Pools

Now examine the nodes in the node pool and gather basic information about their resources.
Two were configured during the deployment.

. Navigate to the node pools and view a node.
[arabic]
.. From the *my-hosted-cluster* overview page, click on the tab for *nodes* at the top.
.. Click on the *first cluster node* to explore it further.
+
image::deploy/cluster_overview_nodes_tab.png[link=self, window=blank, width=100%]

. A new tab will open and bring you to the compute section of your hosted cluster where you can see the node details.
+
image::deploy/hosted_cluster_node_details.png[link=self, window=blank, width=100%]
+
. Close this browser tab and click the *my-hosted-cluster Overview* tab.

. Examine the node pools.
[arabic]
.. Scroll about halfway down the page, near the *Control plane pods* link, and the *Cluster node pools* section.
.. Click on the *three-dot* menu on the right.
.. Select the option for *Manage node pool*.
+
image::deploy/manage_cluster_nodepools.png[link=self, window=blank, width=100%]

. You will be prompted with a pop-up window to *Manage node pool* where you can manually scale the node pool to more than 2 nodes.
[arabic]
.. *Close this window* when you are done viewing it.
+
image::deploy/scale_up_nodepool.png[link=self, window=blank, width=100%]

IMPORTANT: *Do not scale the cluster at this time! We will work with node pools later in the lab.*

== Summary

In this module we have deployed a hosted OpenShift cluster on OpenShift using hosted control planes.
We explored the pods that make up the control plane, and the virtual machines that make up the worker node pool.
