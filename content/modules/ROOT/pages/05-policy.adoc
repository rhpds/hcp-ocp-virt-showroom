= Managing Hosted Clusters using Policies

Governance defines, monitors, and reports whether your cluster is compliant with a policy.
A policy manages Kubernetes custom resource definition instances.
You will use the Red Hat Advanced Cluster Management for Kubernetes policy framework supported policy templates to apply policies managed by these controllers.

Also, a freshly deployed cluster is rather empty.
Usually you will want at least a few operators installed and maybe authentication as well so that your users can start using the cluster.

In this lab you will:

* Set up authorizations for administrative users
* Deploy the following operators to your hosted cluster
** OpenShift Pipelines
** OpenShift GitOps
** OpenShift Serverless

Deploying the *OpenShift GitOps* operator is a good idea because it can also be used by Red Hat Advanced Cluster Management for Kubernetes to deploy and manage applications.

== Set up a ClusterSet

It is good practice to add hosted clusters to a ClusterSet.
ClusterSets allow the grouping of cluster resources.

Most significantly, they enables role-based access control management across all of the resources in the group allowing you to and configure a number of similar clusters at once rather than having to write policies and applications for each individual cluster.

In this lab you create *development* clusters - so our ClusterSet will be named *development*.

While creating our ClusterSet you will also set up Submariner.
Submariner provides direct networking between two or more Kubernetes clusters in a given ManagedClusterSet, either on-premises or in the cloud.

=== Create the ClusterSet

*ManagedClusterSets* are the custom resource definition of ClusterSets.

With your web browser, create the cluster set, named *development*.

Create a binding of this cluster set to a new namespace, named *development-policies*.

More than one managed cluster set may be created and bound to the same namespace.

All policies that appear in the new namespace *development-policies* will apply to the *development* cluster set, and any other cluster sets you may bind to this namespace.

Further, you may create a *placement* to refine the policy assignment to particular managed cluster set, if necessary.

== Set up Administrative Users

In prior sections of this lab you have configured authentication for your hosted cluster.

. In a web browser navigate to the hosted cluster console and log in using *admin* as the username with *openshift* as the password.
+
You will notice that *admin* is just a regular user at this point.

This is because we have not yet created a *ClusterRoleBinding* granting the *admin* user *ClusterAdmin* permissions.

. Create a policy to grant *ClusterAdmin* permission to user *admin*.
This will apply to all clusters in the clusterset "development":
+
.. Click on *Governance -> Create Policy*
+
image::policy/create-policy.png[]

. The Create Policy screen appears.
Fill out the form according to the below, and also not that you'll be editing YAML in the black box as well.
.. Enter the Name: `admin-authorization`
.. Select the Namespace: `development-policies`
.. Click *Next*
+
image::policy/policy-form1.png[]

.. Select the Remediation Action: `Enforce`
.. Paste the following policy-tempalte YAML in the black box, following the existing YAML.
This defines a policy-template that's not predefined for you.
However, you will notice that the form on the left updates to match the policy-template you paste in:
.. Click *Next*
+
image::policy/policy-form2.png[]
+
[source,sh,role=execute]
----
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: admin-authorization
      spec:
        remediationAction: enforce
        severity: medium
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              annotations:
                rbac.authorization.kubernetes.io/autoupdate: "true"
              name: admin-authorization
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: cluster-admin
            subjects:
            - apiGroup: rbac.authorization.k8s.io
              kind: User
              name: admin
----

. A *placement* is optional, and is more fine grained.
It enables particular clusters in a cluster set to be assigned policies.
Otherwise, all clusters in the managed cluster set will be assigned the policy.

.. Click *New placement* and examine the new YAML in the black box.
You will notice that both a *placement* and a *placementbinding* have been created.
+
image::policy/policy-form3.png[]

. *Policy annotations* may be skipped because we are not associating our policies with any security standards in this lab.
.. Click *Next*
+
image::policy/policy-form4.png[]

. Review your policy.
.. Click *Submit* when you're done reviewing.
+
image::policy/policy-form5.png[]

. The *admin-authorization* screen appears.
.. Note that your policy was *Created*, that you have a green check next to *Cluster violations* indicating no violations,
+
image::policy/admin-authorization.png[]

. Find more details about the application of the policy by clicking the *Results* tab.
.. Note that the *clusterrolebindings [admin-authorization] found as specified*.
+
image::policy/admin-authorization-results.png[]

. But the cluster wasn't that way from the start, and you can see the history of the policy by clicking the *View history* on the right.
.. Note that you can see that the cluster was initially in Violation, but that the clusterrolebidning was created successfully and the cluster now has No violations.
+
image::policy/admin-authorization-history.png[]

. View the configuration policy resoruces created.
.. Return to the *Results* tab.
.. Click *View details* and you can see the actual ConfigurationPolicy that governs the resources, in our case a *ClusterRoleBinding*.
+
image::policy/admin-authorization-configuration-policy.png[]

. View the definition of the cluster role binding.
.. Click the *Related resources -> admin-authorization* link.
.. A new tab appears with the Search interface of RHACM open.
.. In it, is the definition of the *admin-authorization* clusterrolebinding on *my-hosted-cluster*.
+
image::policy/admin-authorization-search-crb.png[]

Finally, return to the browser tab with the console of *my-hosted-cluster* and refresh the page.

. Now return to your managed cluster console window and refresh the page. You should now be a full cluster administrator.

== Deploy OpenShift Pipelines Operator

The OpenShift Pipelines Operator is one of the easiest operators to deploy because it only needs a `Subscription` to install the operator - once the operator is running it automatically configures the OpenShift Pipelines deployment on the cluster.

Policies can be used to ensure presence (or absence) of Kubernetes Resources on target clusters.

A `Policy` usually consists of three parts: The `Policy` itself which outlines which resources should (or should not) be on the target clusters. A `Placement` which selects the target clusters and finally a `PlacementBinding` binding the two together.

Note that you could re-use your `Placement` object for multiple policies - but it may be easier to manage to have a separate placement for each policy to enable easier changes in the future.

. Create a policy to install the `Subscription` to a cluster:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: openshift-pipelines-installed
  namespace: development-policies
spec:
  remediationAction: enforce
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: openshift-pipelines-installed
      spec:
        remediationAction: enforce
        pruneObjectBehavior: DeleteIfCreated
        severity: medium
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: operators.coreos.com/v1alpha1
            kind: Subscription
            metadata:
              name: openshift-pipelines
              namespace: openshift-operators
            spec:
              channel: pipelines-1.13
              installPlanApproval: Automatic
              name: openshift-pipelines-operator-rh
              source: redhat-operators
              sourceNamespace: openshift-marketplace
EOF
----

. Create a `Placement` selecting the *development* `ManagedClusterSet`
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: openshift-pipelines-installed
  namespace: development-policies
spec:
  clusterSets:
  - development
EOF
----

. And finally create a `PlacementBinding` to bind the two together and ensure the `Policy` gets deployed to your *development* clusters:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: openshift-pipelines-installed
  namespace: development-policies
placementRef:
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
  name: openshift-pipelines-installed
subjects:
- apiGroup: policy.open-cluster-management.io
  kind: Policy
  name: openshift-pipelines-installed
EOF
----

. This is all that you need to do to install *OpenShift Pipelines* on all our development clusters.
+
Check that the policy has been deployed:
+
[source,sh,role=execute]
----
oc get policy -A | grep pipelines
----
+
.Sample Output
[source,text,options=nowrap]
----
cluster2               development-policies.openshift-pipelines-installed   enforce              Compliant          64s
development-policies   openshift-pipelines-installed                        enforce              Compliant          3m12s
----
+
Note that the policy in the `development-policies` shows as *Compliant* - and that the policy has been copied to the one cluster in your `ManagedClusterSet` - *cluster2*.

== Deploy OpenShift GitOps Operator

The OpenShift GitOps Operator is also one of the easiest operators to deploy because it only needs a `Subscription` to install the operator - once the operator is running it automatically configures the OpenShift GitOps deployment on the cluster.

. Create a policy to install the `Subscription` to a cluster:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: openshift-gitops-installed
  namespace: development-policies
spec:
  remediationAction: enforce
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: openshift-gitops-installed
      spec:
        remediationAction: enforce
        pruneObjectBehavior: DeleteIfCreated
        severity: medium
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: operators.coreos.com/v1alpha1
            kind: Subscription
            metadata:
              name: openshift-gitops-operator
              namespace: openshift-operators
            spec:
              channel: gitops-1.11
              installPlanApproval: Automatic
              name: openshift-gitops-operator
              source: redhat-operators
              sourceNamespace: openshift-marketplace
EOF
----

. Create a `Placement` selecting the *development* `ManagedClusterSet`
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: openshift-gitops-installed
  namespace: development-policies
spec:
  clusterSets:
  - development
EOF
----

. And finally create a `PlacementBinding` to bind the two together and ensure the `Policy` gets deployed to your *development* clusters:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: openshift-gitops-installed
  namespace: development-policies
placementRef:
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
  name: openshift-gitops-installed
subjects:
- apiGroup: policy.open-cluster-management.io
  kind: Policy
  name: openshift-gitops-installed
EOF
----

. This is all that you need to do to install *OpenShift GitOps* on all our development clusters.
+
Check that the policy has been deployed:
+
[source,sh,role=execute]
----
oc get policy -A | grep gitops
----
+
.Sample Output
[source,text,options=nowrap]
----
cluster2               development-policies.openshift-gitops-installed      enforce              Compliant          13s
development-policies   openshift-gitops-installed                           enforce              Compliant          32s
----

== Deploy OpenShift Serverless Operator

The OpenShift Serverless Operator is a little bit more complicated because first you need to deploy the operator by creating a `Subscription`. Then you need to tell the operator to actually install OpenShift Serverless by creating a `KNativeServing` object. In addition you want to create a `KNativeEventing` object to enable event driven architectures.

Both of these objects need to live in their own namespace - so in total you need to create 5 resources via the policy:

* Subscription
* Namespace: knative-serving
* Resource: KNativeServing
* Namespace: knative-eventing
* Resource: KNativeEventing

. Create a policy to install the `Subscription` to a cluster:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: openshift-serverless-installed
  namespace: development-policies
spec:
  remediationAction: enforce
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: openshift-serverless-installed
      spec:
        remediationAction: enforce
        pruneObjectBehavior: DeleteIfCreated
        severity: medium
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: operators.coreos.com/v1alpha1
            kind: Subscription
            metadata:
              name: openshift-serverless-operator
              namespace: openshift-operators
            spec:
              channel: stable
              installPlanApproval: Automatic
              name: serverless-operator
              source: redhat-operators
              sourceNamespace: openshift-marketplace
        - complianceType: musthave
          objectDefinition:
            apiVersion: v1
            kind: Namespace
            metadata:
              name: knative-serving
        - complianceType: musthave
          objectDefinition:
            apiVersion: v1
            kind: Namespace
            metadata:
              name: knative-eventing
        - complianceType: musthave
          objectDefinition:
            apiVersion: operator.knative.dev/v1beta1
            kind: KnativeServing
            metadata:
              name: knative-serving
              namespace: knative-serving
        - complianceType: musthave
          objectDefinition:
            apiVersion: operator.knative.dev/v1beta1
            kind: KnativeEventing
            metadata:
              name: knative-eventing
              namespace: knative-eventing
EOF
----

. Create a `Placement` selecting the *development* `ManagedClusterSet`
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: openshift-serverless-installed
  namespace: development-policies
spec:
  clusterSets:
  - development
EOF
----

. And finally create a `PlacementBinding` to bind the two together and ensure the `Policy` gets deployed to your *development* clusters:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: openshift-serverless-installed
  namespace: development-policies
placementRef:
  apiGroup: cluster.open-cluster-management.io
  kind: Placement
  name: openshift-serverless-installed
subjects:
- apiGroup: policy.open-cluster-management.io
  kind: Policy
  name: openshift-serverless-installed
EOF
----

. This is all that you need to do to install _and configure_ *OpenShift Serverless* on all our development clusters.
+
Check that the policy has been deployed:
+
[source,sh,role=execute]
----
oc get policy -A | grep serverless
----
+
.Sample Output
[source,text,options=nowrap]
----
cluster2               development-policies.openshift-serverless-installed   enforce              NonCompliant       24s
development-policies   openshift-serverless-installed                        enforce              NonCompliant       2m11s
----
+
Note that this time (depending on how quickly you ran the command after creating the policy) policies in the `development-policies` shows as *NonCompliant* - this is because it takes a lot longer to create the subscription - and then create the Serverless resources. After a few minutes the policy will also switch to *Compliant*.

== Summary

In this module you learned:

* How to configure authentication for your managed clusters
* how to create a `ManagedClusterSet` to configure similar clusters as a group
* how to create policies for simple operators to be installed on managed clusters
* how to create a policy for a more complex operator with operands to be installed on managed clusters
