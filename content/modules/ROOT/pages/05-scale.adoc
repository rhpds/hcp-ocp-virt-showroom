= Scaling the Cluster

One of the major benefits of OpenShift on OpenShift with hosted control planes is how easy it is for the cluster node pools to scale dynamically.
As resources are consumed in the hosted cluster, the hosting cluster responds to autoscale requests from the cluster to meet the application needs.

OpenShift Virtualization enables you to dynamically scale your hosted clusters, similarly to how you might have with a cloud hyperscaler like AWS, Azure, etc.

However, your capacity to scale here is limited by the underlying OpenShift Virtualization cluster you are running.
There are features, not covered here, that allow the hosted cluster to use hyperscaler nodes as part of their cluster.
This is more often used to access resources unavailable to you through the hosted cluster - perhaps GPUs or other storage types.

In this module, you will deploy an application to the cluster and use the autoscale feature to dynamically scale the cluster to meet the application needs.

*Goals*

* Deploy an application to the cluster.
* Scale up the application so that it consumes additional resources.
* Observe the cluster autoscale feature.
* Scale the cluster back to two nodes when complete.

[[deploy-app]]
== Deploy an Application to your Hosted Cluster

. If you're not already on your hosted cluster, find the hosted cluster in the Advanced Cluster Management Console and log in.
[arabic]
.. On the upper left drop-down menu, select *All Clusters*.
.. On the left bar, Select *Infrastructure -> Clusters*.
.. Click *my-hosted-cluster*.
+
image::scale/go_to_my-hosted-cluster.png[link=self, window=blank, width=100%]

. Navigate to the *my-hosted-cluster* page.
[arabic]
.. Scroll down to the *Details* section.
... If you have created users per this lab, there will be no credentials available in the RHACM console.
Use username `admin` and password `openshift`.
... If you have NOT created users per this lab, the RHACM console and copy the Username & Password with the *Reveal Credentials* link.
.. Click the *Console URL* link to open a new tab with the hosted cluster console.
+
image::scale/route_to_my-hosted-cluster.png[link=self, window=blank, width=100%]

Create a deployment of the reversewords application in the namespace *default*.
This application will have six replicas, each requiriting 1Gi of memory.
The hosted cluster doesn't have sufficient memory capacity at this time to deploy the application.
We will soon scale up the cluster to acommodate them.
[arabic]
.. On the upper right *Quick create* *+* button, select *Import YAML*.
.. Copy and paste the following YAML into the *Import YAML* dialog box.
.. Click *Create*.
+
[source,sh,role=execute,subs="attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: reversewords
  name: reversewords
  namespace: default
spec:
  replicas: 6
  selector:
    matchLabels:
      app: reversewords
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: reversewords
    spec:
        containers:
        - image: quay.io/rhpds/reversewords:latest
          name: reversewords
          resources:
            requests:
              memory: 1Gi
----
+
image::scale/app_quick_create.png[link=self, window=blank, width=100%]

The Deployment details page appears.

. Investigate the reason that the pods have not deployed.
[arabic]
.. Click on the *Pods* tab.
.. Click on of the :Hourglass: Pending links.
.. The message indicates that the two nodes that are available to deploy application have *Insufficient memory* achieve application deployment.
+
image::scale/pods_in_pending.png[link=self, window=blank, width=100%]

The goal of the next section is to setup automated node scaling, so that the cluster can support the application.

[[explore-autoscale]]
== Explore Autoscaling

Now that the application is deployed and running, set up autoscaling.
If autoscaling is not already enabled, it must be enabled.

. Return to the *Clusters > my-hosted-cluster > Overview* page.
[arabic]
.. Click on the browser tab to return to the *hosting cluster*.
.. On the left bar, select *Infrastructure -> Clusters*, and from the *Cluster list* click *my-hosted-cluster*.
.. Observe that there are still just two nodes in *my-node-pool*, but also see that *Autoscaling* is set to *False*.
+
image::scale/cluster_node_pools.png[link=self, window=blank, width=100%]


. Use the OpenShift Console Web Terminal
[arabic]
.. On the upper-right of the OpenShift Console, click *>_*.
.. Click *OpenShift command line terminal*.
.. The terminal window opens.
+
image::scale/webterminal_start.png[link=self, window=blank, width=100%]

. To enable Autoscaling for *my-node-pool*, apply the following patch to disable the static replica count and enable autoscaling.
[arabic]
.. Copy and paste the following command into the OpenShift command line terminal, and press the enter key.
+
[source,sh,role=execute,subs="attributes"]
----
oc -n clusters patch nodepool my-node-pool --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 6, "min": 2 }}]'
----
+
.Sample output
----
Welcome to the OpenShift Web Terminal. Type "help" for a list of installed CLI tools.
bash-5.1 ~ $ oc -n clusters patch nodepool my-node-pool --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 6, "min": 2 }}]'
nodepool.hypershift.openshift.io/my-node-pool patched
bash-5.1 ~ $
----
+
image::scale/webterminal_nodepool_patch.png[link=self, window=blank, width=100%]

. Close the web terminal.

. Observe the changes to *my-node-pool*.
Autoscaling is now indicating *Min 2 Max 6*.
Over time, *Status* will change from *Pending* to *Ready*.
+
image::scale/nodepool_updating.png[link=self, window=blank, width=100%]

. Virtual Machines are created on the hosting cluster to support the node pool, as triggered by the Pods in pending for the application.
[arabic]
.. On the left bar, click *Virtual Machines*.
.. Watch as VMs are created and adopted by the hosted cluster *my-hosted-cluster* as nodes.
+
image::scale/nodepool_vms.png[link=self, window=blank, width=100%]
+
NOTE: There are usually five VMs created in total.
This is because memory has already been consumed by the operators we installed.
+
. View the application pods and pod details to verify application deployment complete.
[arabic]
.. Return to the browser tab of the hosted cluster, where the application is currently running.
.. On the left bar, click *Workloads -> Deployments*.
.. Click on the link *6 of 6 pods*
+
image::scale/deployments_6_of_6.png[link=self, window=blank, width=100%]
+
[NOTE]
.Awaiting hosted nodes
====
It might take some time for the hosted cluster to detect the VMs as nodes.
Watch the virtual machines become nodes in the *my-node-pool* *Compute -> Nodes* page.

image::scale/await_hosted_nodes.png[link=self, window=blank, width=100%]
====
+
. Find out which nodes each of the pods are running on by managing the columns displayed.
[arabic]
.. Select the *Manage Columns* button.
+
image::scale/manage_columns.png[link=self, window=blank, width=100%]

. Indicate the columns to display.
[arabic]
.. Uncheck *Owner*
.. Check *Node*.
.. Click *Save*.
+
image::scale/manage_columns_details.png[link=self, window=blank, width=100%]

. The pods and the nodes they are deployed on are now displayed.
[arabic]
.. Sort the list by *Node* to see how many nodes are involved support the application.
+
image::scale/pods_nodes_list.png[link=self, window=blank, width=100%]
+
[NOTE]
.Auto Scale Down
====
In this example, the autoscaler initially created three additional virtual machines to deploy the application.
We saw in the previous image that the application pods were deployed to only two nodes.
Once the application was fully deployed, the three virtual machines created were excessive to handle the application's memory needs.
So, eventually, the autoscaler will scale down the number of additional nodes to two in the hosted cluster, leaving us with four total nodes, saving considerable resources.

Notice also that the two original nodes are not affected.

image::scale/auto_scale_down.png[link=self, window=blank, width=100%]
====

Next, we will clean up the application and scale down the cluster.

[[clean-up]]
== Delete the Application and Scale Down the Cluster

Once the additional resources are no longer needed to support the application, the the number of nodes in the hosted cluster can dynamically scale back down to free up resources in the hosting cluster.

. Clean up the application to free up resources.
[arabic]
.. Select the *Actions* drop-down menu from upper right.
.. From the drop-down menu select *Delete Deployment*.
+
image::scale/delete_deployment.png[link=self, window=blank, width=100%]

. Confirm deletion of the the *reversewords* deployment all dependent objects.
[arabic]
.. Click the red *Delete* button.
+
image::scale/confirm_delete.png[link=self, window=blank, width=100%]

. When the application is deleted, the page shows *No Deployments found*.
+
image::scale/no_resources_found.png[link=self, window=blank, width=100%]

. Return to the hosting cluster tab.
[arabic]
.. Close the tab that shows the console of *my-hosted-cluster*.
.. Return to the hosting cluster view of *my-hosted-cluster*.

. There are still four nodes available.
[arabic]
.. Click on the *Nodes* tab at the top of the screen.
+
image::scale/nodes_menu.png[link=self, window=blank, width=100%]

. You need not wait for the nodes to scale down (approximately 10 minutes).
Move on to the next step.
+
image::scale/nodes_menu_2.png[link=self, window=blank, width=100%]

. Turn off the autoscaler and set the number of replicas to 2.
.. On the upper right, click on the *>_* button, which is the OpenShift command line terminal.
.. In the terminal window that opens, paste the following text command.
+
[,sh,role=execute,subs="attributes"]
----
oc -n clusters patch nodepool my-node-pool --type=json -p '[{"op": "remove", "path": "/spec/autoScaling"},{"op":"add", "path": "/spec/replicas", "value": 2}]'
----
+
image::scale/patch_node_pool_autoscale_off.png[link=self, window=blank, width=100%]

The cluster will immediately scale down to two nodes.

[NOTE]
.Manually scaling the NodePool
====
When autoscaling is enabled you will lose the ability to scale worker nodes manually.

However, *if your cluster had not been configured autoscaling,* then it is perfectly acceptable to manually scale it up to three and back down to two nodes to provide the resources you need.
Follow these next steps to perform that action if necessary.

. Manage the NodePool
[arabic]
.. Click on the three-dot menu to the right side of the *Cluster node pools* section
.. When the drop down menu appears click on *Manage node pool.*
+
image::scale/manage_node_pool.png[link=self, window=blank, width=100%]

. Set number of nodes.
[arabic]
.. On the menu that appears, set the number of nodes to two.
.. Click the blue *Update* button.
+
image::scale/two_nodes.png[link=self, window=blank, width=100%]
====

== Summary

In this section we learned about one of the major benefits of hosted control planes and NodePools which is the ability to autoscale up and down on demand when an application requests more resources than are currently available in the cluster, or when an application is deleted and frees up those resources.
