= Creating Users on the Hosted Cluster

One of the primary use cases for a hosted control planes deployment is to provide multitenancy for organizations so that individuals and teams can have their own clusters with dedicated resources that cannot be infringed upon by other users.
Unlike a standard deployment of OpenShift, the configuration for authentication and authorization must be performed on the hosting cluster against the `HostedCluster` object.
In this lab we will step through the process of adding an additional user account for us to login with, grant that user the cluster admin role, and then test that login functionality.

*Goals*

* Configure authentication for a regular user.
* Test the authentication once the cluster is configured.

////
NOTE: In this section we will make use of the Showroom terminal provided to the right of the lab guide to ssh over to our bastion host to perform some command line actions against our cluster.
////

.prepare CLI
----
curl -O https://hcp-cli-download-multicluster-engine.apps.cluster-bb9md.dynamic.redhatworkshops.io/linux/amd64/hcp.tar.gz
mkdir ~/bin
tar -xvzf hcp.tar.gz -C ~/bin/
chmod 755 /home/user/bin/hcp
env -i /home/user/bin/hcp create kubeconfig --name my-hosted-cluster >> my-hosted-cluster.kube
----

.htpasswd-development
----
cat <<'EOF' > .htpasswd-development
admin:$2y$05$dMRkF7f4Wgz/AvBM7UqKm.oucUBbHP3Dai9Qc1k.yKfBIzl3HtP0y
developer:$2y$05$QLjh.UvSYCH2.c7P/CBLMO/gl8f0ywy5b6PPd/k2hPnzEh41xoKiW
EOF
----

.secret
----
oc -n clusters delete secret htpasswd-development
oc -n clusters create secret generic htpasswd-development --from-file=htpasswd=.htpasswd-development
----

.patch oneliner
----
oc -n clusters patch hostedcluster my-hosted-cluster --type=merge --patch='{"spec":{"configuration":{"oauth":{"identityProviders":[{"name":"htpasswd","type":"HTPasswd","htpasswd":{"fileData":{"name": "htpasswd-development"}},"mappingMethod":"claim"}]}}}}'
----

.patch
----
spec:
  configuration:
    oauth:
      identityProviders:
      - htpasswd:
          fileData:
            name: htpasswd-development #secret name
        mappingMethod: claim
        name: htpasswd
        type: HTPasswd
----

.rbac
----
oc adm policy add-cluster-role-to-user cluster-admin admin --kubeconfig my-hosted-cluster.kube
----

[[local-auth]]
== Configure Authentication

=== Gather the Kubeconfig file to interact with the hosted cluster

. Using the Showroom terminal, SSH over to your bastion host at *{bastion_public_hostname}*.
+
* *User:* {bastion_ssh_user_name}
* *Password:* {bastion_ssh_password}
+
image::configure/bastion_login.png[link=self, window=blank, width=100%]

. We are going to use the *hcp* CLI, already installed on the bastion host, to gather the Kubeconfig file from our hosted cluster so we can interact with it via CLI. Copy and paste the following syntax into your console and press Enter.
+
[source,sh,role=execute,subs="attributes"]
----
hcp create kubeconfig --name my-hosted-cluster >> my-hosted-cluster.kube
----
+
image::configure/create_kubeconfig.png[link=self, window=blank, width=100%]

. Use the newly created kubeconfig to check the number of nodes in the hosted cluster node pool to confirm it's working as expected.
+
[source,sh,role=execute,subs="attributes"]
----
oc get nodes --kubeconfig my-hosted-cluster.kube
----
+
image::configure/oc_get_nodes.png[link=self, window=blank, width=100%]

. With the kubeconfig downloaded and confirmed working we can move onto our next steps. Use the *clear* command to clean up the terminal screen.

=== Create User Credentials

. In your terminal copy and paste the following syntax and press the *Enter* key.
+
[source,sh,role=execute,subs="attributes"]
----
htpasswd -c -B -b myuser.htpasswd myuser R3dH4t1!
----
+
image::configure/terminal_create_htpasswd.png[link=self, window=blank, width=100%]

. Use the `cat` command to list the contents of the newly created htpasswd file. Use the syntax below to view the file's contents. It will include our username, and the hashed value of the password we created.
+
[source,sh,role=execute,subs="attributes"]
----
cat myuser.htpasswd
----
+
image::configure/cat_htpasswd.png[link=self, window=blank, width=100%]

. Now we can use this value to create a secret in the cluster, which we will need to be able to log in with our own user account. Copy and paste the following syntax, and press the Enter key.
+
[source,sh,role=execute,subs="attributes"]
----
oc create secret generic htpasswd-mysecret --from-file=htpasswd=myuser.htpasswd -n clusters
----
+
image::configure/secret_created.png[link=self, window=blank, width=100%]

. With the secret created we can now return to our hosting cluster's OpenShift console and to perform the next steps.

=== Add User to Cluster

. Starting from the *Overview* page of our hosting cluster, on the left-side menu click on *Home* and then *API Explorer*.
+
image::configure/home_api_explorer.png[link=self, window=blank, width=100%]

. Use the *Filter by kind* box to search for the term *HostedCluster*. It should return two values, click on the one that shows it's version as *v1beta1*.
+
image::configure/api_explore_hostedcluster.png[link=self, window=blank, width=100%]

. This will bring up the HostedCluster Resource details, click on the *Instances* tab to see our *my-hosted-cluster* deployment.
+
image::configure/hostedcluster_resource.png[link=self, window=blank, width=100%]

. Click on the three-dot menu to the right side of our instance, and select *Edit HostedCluster* from the drop-down menu.
+
image::configure/edit_hostedcluster.png[link=self, window=blank, width=100%]

. Browse to the bottom of the *spec* section and paste in the following syntax to add the *htpasswd* secret as an identity provider. Once complete, click the blue *Save* button.
+
[source,yaml,role=execute]
----
  configuration:
    oauth:
      identityProviders:
      - htpasswd:
          fileData:
            name: htpasswd-mysecret
        mappingMethod: claim
        name: htpasswd
        type: HTPasswd
----
+
image::configure/add_auth_hostedcluster.png[link=self, window=blank, width=100%]

. Once saved you will get two messages, that the *my-hosted-cluster* object has been updated, and a message that invites you to click the *Reload* button to see the new version. Do that.
+
image::configure/saved_auth_hostedcluster.png[link=self, window=blank, width=100%]

. Return to your terminal and run the following command to show the Oauth pods that exist in the *clusters-my-hosted-cluster* namespace. The *oauth-openshift* pods should have all recently restarted.
+
[source,sh,role=execute,subs="attributes"]
----
oc get pods -n clusters-my-hosted-cluster | grep oauth
----
+
image::configure/oauth_pods_restart.png[link=self, window=blank, width=100%]

. Returning to the OpenShift console, scroll up and confirm that your yaml snippet has been applied, and then click the *local-cluster* menu at the top of the page and select *All Clusters* from the dropdown to return to the RHACM Cluster list.
+
image::configure/return_all_clusters.png[link=self, window=blank, width=100%]

. From the list of clusters that appear, click on *my-hosted-cluster*. Then scroll down to the *Details* section.
+
image::configure/all_clusters_list.png[link=self, window=blank, width=100%]

. Do you notice that something is now missing? The credentials for the *kubeadmin* login are now missing since that identity provider has been configured.
+
image::configure/hosted_cluster_creds_missing.png[link=self, window=blank, width=100%]


[[test-auth]]
== Test Authentication

. Click on the *Console URL* link above to launch a new tab where we can test our newly created user account using the username *myuser*, and the password  *R3dH4t1!*. Notice that there is no option to select htpasswd as our Identity Provider as we would expect. Let's attempt to login anyways and see what happens.
+
image::configure/cluster_user_login.png[link=self, window=blank, width=100%]

. When we log in, we find ourselves in the *Developer Perspective* which is the default for accounts created with standard user permissions.
+
image::configure/devel_perspective.png[link=self, window=blank, width=100%]

. Click on the *Skip tour* button to bypass and introduction presented to all new users in OpenShift.
+
image::configure/skip_tour.png[link=self, window=blank, width=100%]

. Over on the left-side menu, click on the *Developer* menu, and select *Administrator* from the drop-down list.
+
image::configure/menu_admin.png[link=self, window=blank, width=100%]

. In the *Administrator* view you will see that we are unable to view practically anything.
This is because we didn't grant our new user account any additional authority over cluster operations.
+
image::configure/blank_admin_view.png[link=self, window=blank, width=100%]

In the following module we will use RHACM Governance Policies to grant our new user account additional authority over cluster operations.

== Summary

In this section we performed configuration of the hosted cluster by creating secrets and updating the HostedCluster custom resource on the hosting cluster, which the hosting cluster then sets up on the hosted cluster.
This shows how OpenShift on OpenShfit clusters using hosted control planes can be easily managed from the hosting cluster after being deployed.

In further modules you will learn how to manager more hosted cluster features using custom resources on the hosting cluster, such as HostedCluster, ClusterSets, and Governance Policies.
