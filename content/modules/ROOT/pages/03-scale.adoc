= Scaling the Cluster

One of the major benefits of OpenShift on OpenShift with hosted control planes is how easy it is for the cluster node pools to scale dynamically.
As resources are consumed in the hosted cluster, the hosting cluster responds to autoscale requests from the cluster to meet the application needs. In this section we will demonstrate this feature.

*Goals*

* Deploy an application to the cluster.
* Scale up the application so that it consumes additional resources.
* Observe the cluster autoscale feature.
* Scale the cluster back to two nodes when complete.

[[deploy-app]]
== Deploy an Application to your Hosted Cluster

. If you're not already on your hosted cluster, find the hosted cluster in the Advanced Cluster Management Console and log in.
[loweralpha]
.. On the upper left drop-down menu, select *All Clusters*.
.. On the left bar, Select *Infrastructure -> Clusters*.
.. Click *my-hosted-cluster*.
+
image::scale/go_to_my-hosted-cluster.png[link=self, window=blank, width=100%]

. Navigate to the *my-hosted-cluster* page.
[loweralpha]
.. Scroll down to the *Details* section.
.. Click the *Console URL* link to open a new tab with the hosted cluster console.
.. If you're not already logged in, return to the RHACM console and copy the Username & Password with the *Reveal Credentials* link.
+
image::scale/route_to_my-hosted-cluster.png[link=self, window=blank, width=100%]

Create a deployment of the reversewords application in the namespace *default*.
This application will have six replicas, each requiriting 1Gi of memory.
The hosted cluster doesn't have sufficient memory capacity at this time to deploy the application.
We will soon scale up the cluster to acommodate them.
[loweralpha]
.. On the upper right *+* button, select *Import YAML*.
.. Copy and paste the following YAML into the *Import YAML* dialog box.
.. Click *Create*.
+
[source,sh,role=execute,subs="attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: reversewords
  name: reversewords
  namespace: default
spec:
  replicas: 6
  selector:
    matchLabels:
      app: reversewords
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: reversewords
    spec:
        containers:
        - image: quay.io/rhpds/reversewords:latest
          name: reversewords
          resources:
            requests:
              memory: 1Gi
----
+
image::scale/app_quick_create.png[link=self, window=blank, width=100%]

The Deployment details page appears.

. Investigate the reason that the pods have not deployed.
[loweralpha]
.. Click on the *Pods* tab.
.. Click on of the :Hourglass: Pending links.
.. The message indicates that the two nodes that are available to deploy application have *Insufficient memory* achieve application deployment.
+
image::scale/pods_in_pending.png[link=self, window=blank, width=100%]

The goal of the next section is to setup automated node scaling, so that the cluster can support the application.

[[explore-autoscale]]
== Explore Autoscaling

Now that the application is deployed and running, set up autoscaling.
If autoscaling is not already enabled, it must be enabled.

. Return to the *Clusters > my-hosted-cluster > Overview* page.
[loweralpha]
.. Click on the browser tab to return to the *hosting cluster*.
.. On the left bar, select *Infrastructure -> Clusters*, and from the *Cluster list* click *my-hosted-cluster*.
.. Observe that there are still just two nodes in *my-node-pool*, but also see that *Autoscaling* is set to *False*.
+
image::scale/cluster_node_pools.png[link=self, window=blank, width=100%]


. Use the OpenShift Console Web Terminal
[loweralpha]
.. SSH to the bastion host once again if needed.
+
image::scale/webterminal_start.png[link=self, window=blank, width=100%]

. To enable Autoscaling for *my-node-pool*, apply the following patch to disable the static replica count and enable autoscaling.
[loweralpha]
.. Copy and paste the following command into the OpenShift command line terminal, and press the enter key.
+
[source,sh,role=execute,subs="attributes"]
----
oc -n clusters patch nodepool my-node-pool --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 6, "min": 2 }}]'
----
+
.Sample output
----
Welcome to the OpenShift Web Terminal. Type "help" for a list of installed CLI tools.
bash-5.1 ~ $ oc -n clusters patch nodepool my-node-pool --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 6, "min": 2 }}]'
nodepool.hypershift.openshift.io/my-node-pool patched
bash-5.1 ~ $
----
+
image::scale/webterminal_nodepool_patch.png[link=self, window=blank, width=100%]

. Close the web terminal.

. Observe the changes to *my-node-pool*.
Autoscaling is now indicating *Min 2 Max 6*.
Over time, *Status* will change from *Pending* to *Ready*.
+
image::scale/nodepool_updating.png[link=self, window=blank, width=100%]

. Virtual Machines are created on the hosting cluster to support the node pool, as triggered by the Pods in pending for the application.
[loweralpha]
.. On the left bar, click *Virtual Machines*.
.. Watch as VMs are created and adopted by the hosted cluster *my-hosted-cluster* as nodes.
+
image::scale/nodepool_vms.png[link=self, window=blank, width=100%]

. View the application pods and pod details to verify application deployment complete.
[loweralpha]
.. Return to the browser tab of the hosted cluster, where the application is currently running.
.. On the left bar, click *Workloads -> Deployments*.
.. Click on the link *6 of 6 pods*
+
image::scale/deployments_6_of_6.png[link=self, window=blank, width=100%]

. Find out which nodes each of the pods are running on by managing the columns displayed.
[loweralpha]
.. Select the *Manage Columns* button.
+
image::scale/manage_columns.png[link=self, window=blank, width=100%]

. Indicate the columns to display.
[loweralpha]
.. Uncheck *Owner*
.. Check *Node*.
.. Click *Save*.
+
image::scale/manage_columns_details.png[link=self, window=blank, width=100%]

. The pods and the nodes they are deployed on are now displayed.
[loweralpha]
.. Sort the list by *Node* to see how many nodes are involved support the application.
+
image::scale/pods_nodes_list.png[link=self, window=blank, width=100%]

Next, we will clean up the application and scale down the cluster.

[[clean-up]]
== Delete the Application and Scale Down the Cluster

Once the additional resources are no longer needed to support the application, the the number of nodes in the hosted cluster can dynamically scale back down to free up resources in the hosting cluster.

. Clean up the application to free up resources.
[loweralpha]
.. Select the *Actions* drop-down menu from upper right.
.. From the drop-down menu select *Delete Deployment*.
+
image::scale/delete_deployment.png[link=self, window=blank, width=100%]

. Confirm deletion of the the *reversewords* deployment all dependent objects.
[loweralpha]
.. Click the red *Delete* button.
+
image::scale/confirm_delete.png[link=self, window=blank, width=100%]

. When the application is deleted, the page shows *No Deployments found*.
+
image::scale/no_resources_found.png[link=self, window=blank, width=100%]

. Return to the hosting cluster tab.
[loweralpha]
.. Close the tab that shows the console of *my-hosted-cluster*.
.. Return to the hosting cluster *local-cluster - Overview*.

. There are still three nodes available.
[loweralpha]
.. Click on the *Nodes* tab at the top of the screen.
+
image::scale/nodes_menu.png[link=self, window=blank, width=100%]

. Wait patiently for the nodes to scale down (approximately 10 minutes) and then check again.
+
image::scale/nodes_menu_2.png[link=self, window=blank, width=100%]

[NOTE]
.Manually scaling the NodePool
====
When autoscaling is enabled you will lose the ability to scale worker nodes manually.

However, if your cluster had not been configured autoscaling, then it is perfectly acceptable to manually scale it up to three and back down to two nodes to provide the resources you need.
Follow these next steps to perform that action if necessary.

. Manage the NodePool
[loweralpha]
.. Click on the three-dot menu to the right side of the *Cluster node pools* section
.. When the drop down menu appears click on *Manage node pool.*
+
image::scale/manage_node_pool.png[link=self, window=blank, width=100%]

. Set number of nodes.
[loweralpha]
.. On the menu that appears, set the number of nodes to two.
.. Click the blue *Update* button.
+
image::scale/two_nodes.png[link=self, window=blank, width=100%]
====

== Summary

In this section we learned about one of the major benefits of hosted control planes and NodePools which is the ability to autoscale up and down on demand when an application requests more resources than are currently available in the cluster, or when an application is deleted and frees up those resources.
